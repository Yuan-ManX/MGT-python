
@inproceedings{jensenius_using_2006,
	address = {New Orleans, LA},
	title = {Using motiongrams in the study of musical gestures},
	copyright = {All rights reserved},
	url = {https://www.duo.uio.no/handle/10852/26923},
	abstract = {Navigating hours of video material is often time-consuming, and traditional keyframe displays are not particularly useful when studying single-shot studio recordings of music-related movement. This paper presents the idea of motiongrams and how we use such displays in our studies of dancers' free move- ments to music.},
	urldate = {2015-02-26},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	author = {Jensenius, Alexander Refsum},
	year = {2006},
	note = {00021},
	pages = {499--502},
	file = {Snapshot:/home/alexander/Zotero database/storage/SXM679N4/26923.html:text/html}
}

@inproceedings{jensenius_video_2010,
	address = {Trondheim},
	title = {A video based analysis system for realtime control of concatenative sound synthesis and spatialisation},
	copyright = {All rights reserved},
	url = {http://urn.nb.no/URN:NBN:no-28129},
	abstract = {We report on the development of a video based analysis system that controls concatenative sound synthesis and sound spatialisation in realtime in concert performances. The system has been used in several pieces, most recently Transformation for electric violin and live electronics, where the performer controls sound playback through motion on stage.},
	booktitle = {Proceedings of {Norwegian} {Artificial} {Intelligence} {Symposium}},
	publisher = {Tapir Akademisk Forlag},
	author = {Jensenius, Alexander Refsum and Johnson, Victoria},
	editor = {Yildirim, Sule and Kofod-Petersen, Andersen},
	year = {2010},
	pages = {85--88},
	file = {Jensenius and Johnson - 2010 - A video based analysis system for realtime control.pdf:/home/alexanje/Drive-UiO/reference/Zotero/Jensenius_Johnson/Jensenius and Johnson - 2010 - A video based analysis system for realtime control.pdf:application/pdf}
}

@article{jensenius_sonifying_2013,
	title = {Sonifying the shape of human body motion using motiongrams},
	volume = {8},
	copyright = {All rights reserved},
	url = {http://urn.nb.no/URN:NBN:no-40244},
	abstract = {The paper presents sonomotiongram, a technique for the creation of auditory displays of human body motion based on motiongrams. A motiongram is a visual display of motion, based on frame differencing and reduction of a regular video recording. The resultant motiongram shows the spatial shape of the motion as it unfolds in time, somewhat similar to the way in which spectrograms visualise the shape of (musical) sound. The visual similarity of motiongrams and spectrograms is the conceptual starting point for the sonomotiongram technique, which explores how motiongrams can be turned into sound using “inverse FFT”. The paper presents the idea of shape-sonification, gives an overview of the sonomotiongram technique, and discusses sonification examples of both simple and complex human motion.},
	number = {2},
	journal = {Empirical Musicology Review},
	author = {Jensenius, Alexander Refsum and Godøy, Rolf Inge},
	year = {2013},
	pages = {73--83}
}

@inproceedings{jensenius_non-realtime_2013,
	address = {Stockholm},
	title = {Non-{Realtime} {Sonification} of {Motiongrams}},
	copyright = {All rights reserved},
	url = {http://urn.nb.no/URN:NBN:no-37228},
	abstract = {The paper presents a non-realtime implementation of the sonomotiongram method, a method for the sonification of motiongrams. Motiongrams are spatiotemporal displays of motion from video recordings, based on frame-differencing and reduction of the original video recording. The sonomotiongram implementation presented in this paper is based on turning these visual displays of motion into sound using FFT filtering of noise sources. The paper presents the application ImageSonifyer, accompanied by video examples showing the possibilities of the sonomotiongram method for both analytic and creative applications.},
	booktitle = {Proceedings of {Sound} and {Music} {Computing}},
	author = {Jensenius, Alexander Refsum},
	year = {2013},
	pages = {500--505}
}

@inproceedings{jensenius_evaluating_2012,
	address = {Copenhagen},
	title = {Evaluating {How} {Different} {Video} {Features} {Influence} the {Visual} {Quality} of {Resultant} {Motiongrams}},
	copyright = {All rights reserved},
	url = {http://urn.nb.no/URN:NBN:no-31294},
	abstract = {Motiongrams are visual representations of human motion, generated from regular video recordings. This paper evaluates how different video features may influence the generated motiongram: inversion, colour, filtering, background, lighting, clothing, video size and compression. It is argued that the proposed motiongram implementation is capable of visualising the main motion features even with quite drastic changes in all of the above mentioned variables.},
	booktitle = {Proceedings of the {Sound} and {Music} {Computing} {Conference}},
	author = {Jensenius, Alexander Refsum},
	year = {2012},
	pages = {467--472},
	file = {Jensenius_2012_Evaluating_How_Different_Video_Features.pdf:/home/alexarje/Dropbox/Reference/Zotero/Jensenius/Jensenius_2012_Evaluating_How_Different_Video_Features.pdf:application/pdf}
}

@incollection{jensenius_experimental_2014,
	address = {Oslo},
	title = {From experimental music technology to clinical tool},
	copyright = {All rights reserved},
	url = {http://urn.nb.no/URN:NBN:no-46186},
	abstract = {Human body motion is integral to all parts of musical experience, from performance to perception. But how is it possible to study body motion in a systematic manner? This article presents a set of video-based visualisation techniques developed for the analysis of music-related body motion, including motion images, motion-history images and motiongrams. It includes examples of how these techniques have been used in studies of music and dance performances, and how they, quite unexpectedly, have become useful in laboratory experiments on ADHD and clinical studies of CP. Finally, it includes reflections regarding what music researchers can contribute to the study of human motion and behaviour in general.},
	booktitle = {Music, health, technology, and design},
	publisher = {Norwegian Academy of Music},
	author = {Jensenius, Alexander Refsum},
	editor = {Stens{\textbackslash}a eth, Karette},
	year = {2014}
}

@article{jensenius_video_2013,
	title = {Some video abstraction techniques for displaying body movement in analysis and performance},
	volume = {46},
	copyright = {All rights reserved},
	url = {http://urn.nb.no/URN:NBN:no-38076},
	doi = {10.1162/LEON_a_00485},
	abstract = {This paper presents an overview of techniques for creating visual displays of human body movement based on video recordings. First a review of early movement and video visualization techniques is given. Then follows an overview of techniques that the author has developed and used in the study of music-related body movements: motion history images, motion average images, motion history keyframe images and motiongrams. Finally, examples are given of how such visualization techniques have been used in empirical music research, in medical research and for creative applications.},
	number = {1},
	journal = {Leonardo},
	author = {Jensenius, Alexander Refsum},
	year = {2013},
	pages = {53--60}
}

@phdthesis{jensenius_actionsound:_2007,
	type = {{PhD} thesis},
	title = {Action–{Sound}: {Developing} {Methods} and {Tools} to {Study} {Music}-{Related} {Body} {Movement}},
	copyright = {All rights reserved},
	url = {http://urn.nb.no/URN:NBN:no-18922},
	abstract = {Body movement is integral to both performance and perception of music, and this dissertation suggests that we also think about music as movement. Based on ideas of embodied music cognition, it is argued that ecological knowledge of action-sound couplings guide our experience of music, both in perception and performance. Then follows a taxonomy of music-related body movements, before various observation studies of perceiver's music-movement correspondences are presented: air instrument performance, free dance to music, and sound-tracing. These studies showed that both novices and experts alike seem to associate various types of body movement with features in the musical sound. Knowledge from the observation studies was used in the exploration of artificial action- sound relationships through the development of various prototype music controllers, including the Cheapstick, music balls, and the Music Troll. This exploration showed that it is possible to create low-cost and human-friendly music controllers that may be both intuitive and creatively interesting. The last part of the dissertation presents tools and methods that have been developed throughout the project, including the Musical Gestures Toolbox for the graphical programming environment Max/MSP/Jitter; techniques for creating motion history images and motiongrams of video material; and development of the Gesture Description Interchange Format (GDIF) for streaming and storing music-related movement data. These tools may be seen as an answer to many of the research questions posed in the dissertation, and have facilitated the analysis of music-related movement and creation of artificial action-sound relationships in the project.},
	school = {University of Oslo},
	author = {Jensenius, Alexander Refsum},
	year = {2007},
	file = {Snapshot:/home/alexander/Zotero database/storage/WHCERA89/27149.html:text/html}
}

@incollection{jensenius_methods_2018,
	address = {Berlin Heidelberg},
	title = {Methods for studying music-related body motion},
	copyright = {All rights reserved},
	isbn = {978-3-662-55002-1},
	url = {http://urn.nb.no/URN:NBN:no-68158},
	abstract = {This chapter presents an overview of some methodological approaches and technologies that can be used in the study of music-related body motion. The aim is not to cover all possible approaches, but rather to highlight some of the ones that are more relevant from a musicological point of view. This includes methods for video-based and sensor-based motion analyses, both qualitative and quantitative. It also includes discussions of the strengths and weaknesses of the different methods, and reflections on how the methods can be used in connection to other data in question, such as physiological or neurological data, symbolic notation, sound recordings and contextual data.},
	booktitle = {Handbook of {Systematic} {Musicology}},
	publisher = {Springer-Verlag},
	author = {Jensenius, Alexander Refsum},
	editor = {Bader, Rolf},
	year = {2018},
	pages = {567--580}
}

@inproceedings{jensenius_musical_2018,
	address = {Paris, France},
	title = {The {Musical} {Gestures} {Toolbox} for {Matlab}},
	url = {http://urn.nb.no/URN:NBN:no-68201},
	abstract = {The Musical Gestures Toolbox for Matlab (MGT) aims at assisting music researchers with importing, preprocessing,
analyzing, and visualizing video, audio, and motion capture data in a coherent manner within Matlab.},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval}, {Late}-{Breaking} {Demos}},
	author = {Jensenius, Alexander Refsum},
	year = {2018},
	file = {Jensenius_2018_The Musical Gestures Toolbox for Matlab.pdf:/home/alexanje/Dropbox (UiO)/reference/Zotero/Jensenius/Jensenius_2018_The Musical Gestures Toolbox for Matlab.pdf:application/pdf}
}